{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = os.listdir()    \n",
    "csv_files = list(filter(lambda f: f.endswith('.csv'), all_files))\n",
    "dfs = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    dfs.append(pd.read_csv(csv_file))\n",
    "\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "df.to_csv(\"camping_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_campings(src, dest):\n",
    "    def clean_line(line, references):\n",
    "        for key in line.keys():\n",
    "            if str(line[key]) == 'nan':\n",
    "                line[key] = ''\n",
    "\n",
    "            line['localite'] = line['localite'].split(',')[0].strip()\n",
    "            if line['localite'] in references.keys() and type(references[line['localite']]['station']) == str:\n",
    "                line['nom_station'] = references[line['localite']]['station']\n",
    "\n",
    "            if type(line['nom_station']) != str:\n",
    "                line['nom_station'] = \"\"\n",
    "\n",
    "            line['web-scrapper-order'] = \"\"\n",
    "            line['n_offre'] = \"\"\n",
    "            line['date_debut-jour'] = \"\"\n",
    "            line['localite'] = line['localite'].replace(', ', ' ') if not line['localite'].split(' ')[0].isdigit() else ' '.join(line['localite'].split(' ')[1:]).replace(', ', ' ')\n",
    "            line['typologie'] = line['typologie'].replace(', ', ' - ')\n",
    "            line['prix_init'] = str(int(float(line['prix_init']))) if line['prix_init'] != 'prix_init' else 'prix_init'\n",
    "            line['prix_actuel'] = str(int(float(line['prix_actuel']))) if line['prix_actuel'] != 'prix_actuel' else 'prix_actuel'\n",
    "\n",
    "        return line\n",
    "    \n",
    "\n",
    "\n",
    "    csv_source = pd.read_csv(src, encoding='utf-8')\n",
    "    csv_source.dropna(subset=['Nb personnes'], inplace=True)\n",
    "    csv_source = csv_source[csv_source['Nb personnes'].isin(['4 Adultes','6 Adultes', '8 Adultes'])]\n",
    "    csv_source.drop_duplicates(inplace=True, subset=['date_price', 'date_debut', 'date_fin', 'prix_init', 'prix_actuel', 'typologie', 'nom', 'localite'])\n",
    "    csv_source.sort_values(inplace=True, ascending = True, by=['Nb semaines', 'date_debut'])\n",
    "    csv_dict = csv_source.to_dict(orient='records')\n",
    "\n",
    "    new_dict = []\n",
    "\n",
    "    all_references_list = pd.read_excel('referencement stations.xlsm', sheet_name='Feuil1').to_dict(orient='records')\n",
    "\n",
    "    all_references_dict = {}\n",
    "    for ref in all_references_list:\n",
    "        if ref['Localite'] not in all_references_dict.keys():\n",
    "            all_references_dict[ref['Localite'].split(',')[0]] = {'station': ref['Station'], 'dest': ref['Cle station']}\n",
    "\n",
    "    for line in csv_dict:\n",
    "        cleaned_line = clean_line(line, all_references_dict)\n",
    "        new_dict.append(cleaned_line)\n",
    "\n",
    "    with open(dest, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['web-scrapper-order', 'date_price', 'date_debut', 'date_fin', 'prix_init', 'prix_actuel', 'typologie', 'Nb personnes','n_offre', 'nom', 'localite', 'date_debut-jour','Nb semaines', 'cle_station', 'nom_station']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in new_dict:\n",
    "            try:\n",
    "                writer.writerow(line)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "def merge_campings(srcs, dest):\n",
    "    csv_list = []\n",
    "\n",
    "    csv_headers = pd.DataFrame(columns=[\n",
    "    'web-scrapper-order',\n",
    "    'date_price',\n",
    "    'date_debut',\n",
    "    'date_fin',\n",
    "    'prix_init',\n",
    "    'prix_actuel',\n",
    "    'Nb personnes',\n",
    "    'typologie',\n",
    "    'n_offre',\n",
    "    'nom',\n",
    "    'localite',\n",
    "    'date_debut-jour',\n",
    "    'Nb semaines'\n",
    "    ])\n",
    "\n",
    "    csv_list.append(csv_headers)\n",
    "\n",
    "    for file in srcs:\n",
    "        csv_list.append(pd.read_csv(file, encoding='utf-8'))\n",
    "\n",
    "        csv_merged = pd.concat(csv_list)\n",
    "        csv_merged.sort_values(inplace=True, ascending = True, by=['Nb semaines', 'date_debut'])\n",
    "        csv_merged.drop_duplicates(subset=['date_price', 'date_debut', 'date_fin', 'prix_init', 'prix_actuel', 'typologie', 'nom', 'localite'])\n",
    "        csv_merged.to_csv(dest, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here x should be replaced with de date of monday of the week scrap\n",
    "clean_campings('camping_all.csv', 'camping_cleaned_02_10_2023.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
